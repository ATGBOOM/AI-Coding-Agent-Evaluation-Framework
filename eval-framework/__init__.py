"""
LLM Coding Agent Evaluation Framework

A framework for evaluating LLM coding agents using HumanEval benchmark
across correctness, explainability, and design adherence dimensions.
"""

__version__ = "0.1.0"
